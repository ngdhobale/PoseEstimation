# -*- coding: utf-8 -*-
"""PoseEstimation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A0Ezk0o6s9tpE7qwFOJ34m53bNe70Om5
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

#stop_words = set(stopwords.words('english'))

lemmatizer = WordNetLemmatizer()
stop_words_isl = {"a", "an" ,"the", "is", "am", "are"}


# Define function to lemmatize each word with its POS tag
 
# POS_TAGGER_FUNCTION : TYPE 1
def pos_tagger(word,nltk_tag):
    if word.endswith("ing"):
        return wordnet.VERB
    if nltk_tag.startswith('J'):
        return wordnet.ADJ
    elif nltk_tag.startswith('V'):
        return wordnet.VERB
    elif nltk_tag.startswith('N'):
        return wordnet.NOUN
    elif nltk_tag.startswith('R'):
        return wordnet.ADV
    else:         
        return None

    
# sent_tokenize is one of instances of 
# PunktSentenceTokenizer from the nltk.tokenize.punkt module
txt="how are you?"  
tokenized = sent_tokenize(txt)

lemmatizer = WordNetLemmatizer()
lemmatized_sentence = []

for i in tokenized:
      
    # Word tokenizers is used to find the words 
    # and punctuation in a string
    wordsList = nltk.word_tokenize(i)
    
    # removing stop words from wordList
    wordsList = [w for w in wordsList if not w in stop_words_isl] 


    #  Using a Tagger. Which is part-of-speech 
    # tagger or POS-tagger. 
    tagged = nltk.pos_tag(wordsList)
  
    #print(tagged)
    # we use our own pos_tagger function to make things simpler to understand.
    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[0],x[1])), tagged))
    #print(wordnet_tagged)

    lemmitization_dict = dict()

    for word, tag in wordnet_tagged:
        if tag is None:
            # if there is no available tag, append the token as is
            lemmatized_sentence.append(word)
        else:       
            # else use the tag to lemmatize the token
            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))
            lemmitization_dict[word] = lemmatizer.lemmatize(word, tag)

    
    print(lemmatized_sentence)
    #print(lemmitization_dict)

import spacy
nlp = spacy.load("en_core_web_sm")



# object and subject constants
OBJECT_DEPS = {"dobj", "dative", "attr", "oprd","pobj"}
SUBJECT_DEPS = {"nsubj", "nsubjpass", "csubj","csubjpass", "agent", "expl", "acomp", "avmod"}
# tags that define wether the word is wh-
WH_WORDS = {"WP", "WP$", "WRB"}

# gather the user input and gather the info
doc = nlp(txt)

# extract the subject, object and verb from the input
def extract_svo(doc):
    sub = []
    at = []
    ve = []
    for token in doc:
        # is this a verb?
        if token.pos_ == "VERB":
            ve.append(token.text)
        # is this the object?
        if token.dep_ in OBJECT_DEPS or token.head.dep_ in OBJECT_DEPS:
            at.append(token.text)
        # is this the subject?
        if token.dep_ in SUBJECT_DEPS or token.head.dep_ in SUBJECT_DEPS or token.pos_ == "ADJ":
            sub.append(token.text)
    return " ".join(sub).strip().lower(), " ".join(ve).strip().lower(), " ".join(at).strip().lower()

# wether the doc is a question, as well as the wh-word if any
def is_question(doc):
    # is the first token a verb?
    if len(doc) > 0 and doc[0].pos_ == "VERB":
        return True, ""
    # go over all words
    for token in doc:
        # is it a wh- word?
        if token.tag_ in WH_WORDS:
            return True, token.text.lower()
    return False, ""


    
# print out the pos and deps
for token in doc:
    print("Token {} POS: {}, dep: {}".format(token.text, token.pos_, token.dep_))

# get the input information
subject, verb, attribute = extract_svo(doc)
question, wh_word = is_question(doc)
print("SVO:, Subject: {}, Verb: {}, Object: {}, Question: {}, Wh_word: {}".format(subject, verb, attribute, question, wh_word))

if("which" in subject):
    txt_sov = " ".join([attribute.replace(wh_word,""), verb, subject.replace(wh_word,""), wh_word]) 
elif(wh_word is not None):
    txt_sov = " ".join([subject.replace(wh_word,""), attribute.replace(wh_word,""), verb, wh_word]) 
else:
    txt_sov = " ".join([subject,attribute, verb])

for key, value in lemmitization_dict.items():
    txt_sov = txt_sov.replace(key, value)

isl_list = [w for w in list(txt_sov.split(" ")) if not w in stop_words_isl]

#print(isl_list)
isl = ""
for i in isl_list:
    if i != "":
        isl = isl+" "+i

print(isl)
# output stored in isl

"""Install and import dependencies"""

import mediapipe as mp
import numpy as np
import cv2

mp_drawing = mp.solutions.drawing_utils
mp_holistic = mp.solutions.holistic

"""2. Make Detections from Feed <br>
Detect Facial Landmarks <br>
Detect Hand Poses <br>
Detect Body Poses <br>
"""

for i in isl_list:
    cap = cv2.VideoCapture('Dataset/'+i+'.mp4')
    # Initiate holistic model
    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
        while cap.isOpened():
            ret, frame = cap.read()
            
            img = np.zeros([720,1290,3],dtype=np.uint8)
            img.fill(255)
            cv2.putText(img=img, text=i, org=(100, 100), fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=2, color=(0, 0, 0),thickness=2)
            
            if ret==False:
                break
            # Recolor Feed
            
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # Make Detections
            results = holistic.process(image)
            # print(results.face_landmarks)
        
            # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks
        
            # Recolor image back to BGR for rendering
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            # 2. Right hand
            mp_drawing.draw_landmarks(img, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, 
                                 mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=2),
                                 mp_drawing.DrawingSpec(color=(0,0,0), thickness=8, circle_radius=2)
                                 )
            # 3. Left Hand
            mp_drawing.draw_landmarks(img, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, 
                                 mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=2),
                                 mp_drawing.DrawingSpec(color=(0,0,0), thickness=8, circle_radius=2)
                                 )

            # 4. Pose Detectionsq
            mp_drawing.draw_landmarks(img, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, 
                                 mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=2),
                                 mp_drawing.DrawingSpec(color=(0,0,0), thickness=8, circle_radius=2)
                                 )
        
            cv2.imshow('Sign Translation', img)
            
            if cv2.waitKey(30) & 0xFF == ord('q'):
                break
    cap.release()
    cv2.destroyAllWindows()

import mediapipe as mp
import numpy as np
import cv2


mp_drawing = mp.solutions.drawing_utils
mp_holistic = mp.solutions.holistic

cap = cv2.VideoCapture(0)
    # Initiate holistic model
with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    while cap.isOpened():
        ret, frame = cap.read()
            
        img = np.zeros([720,1290,3],dtype=np.uint8)
        img.fill(255)
        #cv2.putText(img=img, text=i, org=(100, 100), fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=2, color=(0, 0, 0),thickness=2)
            
        if ret==False:
            break
            # Recolor Feed
            
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # Make Detections
        results = holistic.process(image)
            # print(results.face_landmarks)
        
            # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks
        
            # Recolor image back to BGR for rendering
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            # 2. Right hand
        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, 
                                 mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=2),
                                 mp_drawing.DrawingSpec(color=(0,0,0), thickness=8, circle_radius=2)
                                 )
            # 3. Left Hand
        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, 
                                 mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=2),
                                 mp_drawing.DrawingSpec(color=(0,0,0), thickness=8, circle_radius=2)
                                 )

            # 4. Pose Detectionsqgg
        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, 
                                 mp_drawing.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=2),
                                 mp_drawing.DrawingSpec(color=(0,0,0), thickness=8, circle_radius=2)
                                 )
        
        cv2.imshow('Sign Translation', image)
            
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break
cap.release()
cv2.destroyAllWindows()